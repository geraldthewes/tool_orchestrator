# ToolOrchestra Configuration
# ===========================
#
# Copy this file to config/config.yaml and update values with your actual endpoints.
#
# IMPORTANT:
# - config/config.yaml contains secrets (API keys, Langfuse credentials)
# - It is gitignored - do not commit it to version control
# - Run `make push-config` to push to Consul after editing
# - Container fetches config from Consul at startup

version: "1.0"

# Global max_tokens for LLM output. Must be less than model context window minus input tokens.
# Nemotron-Orchestrator-8B has 40K total context window.
max_tokens: 8192

# =============================================================================
# ORCHESTRATOR MODEL
# =============================================================================
# The main orchestrator LLM endpoint (vLLM/SGLang/any OpenAI-compatible)

orchestrator:
  base_url: "http://localhost:8001/v1"
  model: "nvidia/Nemotron-Orchestrator-8B"
  temperature: 0.7
  max_steps: 10
  context_length: 32768  # Qwen3-8B base context window
  # Generation parameters to prevent repetitive output
  # Nemotron-Orchestrator-8B is Qwen3-derived; these are its stop tokens
  stop:
    - "<|im_end|>"
    - "<|endoftext|>"
  frequency_penalty: 0.3  # Discourage repeating tokens (0.0-2.0)
  presence_penalty: 0.1   # Discourage reusing tokens (0.0-2.0)
  # Observation buffer token budgets
  max_observation_tokens: 2048   # Max tokens per individual observation
  attempts_budget: 8000          # Token budget for attempt history
  code_budget: 12000             # Token budget for code/calculation results
  delegates_budget: 8000         # Token budget for delegate responses
  # Context externalization (RLM-inspired)
  # When delegate responses exceed the threshold, they are stored externally
  # and replaced with summaries + reference IDs for selective retrieval
  externalize_threshold: 4000    # Chars above which to externalize delegate content
  keep_recent_delegate_full: 1   # Keep last N delegate calls in full (not externalized)

# =============================================================================
# API SERVER
# =============================================================================

server:
  host: "0.0.0.0"
  port: 8000
  workers: 1
  reload: false  # Enable auto-reload for development

# =============================================================================
# TOOLS
# =============================================================================

tools:
  # SearXNG Web Search
  searxng:
    url: "http://searxng-cache.cluster:9999/search"
    timeout: 30

  # Remote Python Executor (sandboxed code execution service)
  python_executor:
    url: "http://pyexec.cluster:9999/"
    timeout: 30

# =============================================================================
# RUNTIME SETTINGS
# =============================================================================

# Fast-path routing for simple queries (bypass orchestration for simple queries)
# When enabled, simple queries like "hello" or "suggest follow-up questions"
# are routed directly to Fast LLM instead of going through the full ReAct loop
fast_path:
  enabled: true

# Logging level (DEBUG, INFO, WARNING, ERROR)
logging:
  level: "INFO"

# =============================================================================
# LANGFUSE OBSERVABILITY
# =============================================================================
# Tracing enables when both keys are provided.
# Get keys from https://cloud.langfuse.com or your self-hosted instance.

langfuse:
  enabled: true
  public_key: "pk-lf-your-public-key"
  secret_key: "sk-lf-your-secret-key"
  host: "http://langfuse.cluster:9999"  # Or https://cloud.langfuse.com
  flush_at: 10
  flush_interval: 1.0
  debug: false
  output_max_length: 0  # 0 = unlimited

# =============================================================================
# DSPY OPTIMIZATION
# =============================================================================
# Path to load optimized DSPy prompts from (optional)
# When set, DSPy modules will load optimized prompts from this path

dspy:
  optimized_prompts_path: ""

# =============================================================================
# DELEGATE LLMs
# =============================================================================
# Each delegate becomes a tool named "ask_{role}" (e.g., ask_reasoner).
# Update base_url and model with your actual endpoints.

delegates:
  reasoner:
    display_name: "Reasoning Expert"
    connection:
      type: "openai_compatible"
      base_url: "http://reasoning-llm.cluster:8000/v1"
      model: "gpt-oss-120b"
      # api_key: ""  # Optional, for authenticated endpoints
    capabilities:
      context_length: 131072
      max_output_tokens: 32768
      specializations:
        - "complex_reasoning"
        - "multi_step_analysis"
        - "detailed_explanations"
        - "research_synthesis"
    defaults:
      temperature: 0.7
      max_tokens: 16384
      timeout: 600  # 10 minutes for complex reasoning
    description: "Delegate complex reasoning tasks requiring deep analysis, multi-step thinking, or synthesis of multiple concepts."

  coder:
    display_name: "Coding Expert"
    connection:
      type: "openai_compatible"
      base_url: "http://coding-llm.cluster:8000/v1"
      model: "qwen3-coder"
    capabilities:
      context_length: 131072
      max_output_tokens: 32768
      specializations:
        - "code_generation"
        - "debugging"
        - "code_review"
        - "refactoring"
        - "technical_documentation"
    defaults:
      temperature: 0.3
      max_tokens: 16384
      timeout: 300  # 5 minutes for code generation
    description: "Delegate coding tasks including code generation, debugging, code review, and technical implementations."

  fast:
    display_name: "Fast Responder"
    connection:
      type: "openai_compatible"
      base_url: "http://ollama.cluster:11434/v1"
      model: "qwen3:8b"
    capabilities:
      context_length: 16384
      max_output_tokens: 8192
      specializations:
        - "quick_answers"
        - "simple_reasoning"
        - "summarization"
        - "classification"
    defaults:
      temperature: 0.7
      max_tokens: 2048
      timeout: 120  # 2 minutes for quick responses
    description: "Delegate simple tasks requiring fast responses. Good for quick reasoning, simple Q&A, and time-sensitive queries."
