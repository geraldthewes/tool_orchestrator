# ToolOrchestra Environment Configuration
# Copy this file to .env and update values as needed

# =============================================================================
# ORCHESTRATOR MODEL
# =============================================================================
# The main orchestrator LLM endpoint (vLLM/SGLang/any OpenAI-compatible)
ORCHESTRATOR_BASE_URL=http://localhost:8001/v1
ORCHESTRATOR_MODEL=nvidia/Nemotron-Orchestrator-8B

# =============================================================================
# DELEGATE LLMs
# =============================================================================
# Delegates are configured via config/delegates.yaml
# Environment variables below can override values in the YAML config

# Path to delegates YAML configuration (optional, defaults to config/delegates.yaml)
# DELEGATES_CONFIG_PATH=config/delegates.yaml

# Reasoning LLM - For complex reasoning and analysis tasks
REASONING_LLM_BASE_URL=http://gx10-d8ce.cluster:8000/v1
REASONING_LLM_MODEL=glm-reap
# REASONING_LLM_TIMEOUT=600  # Request timeout in seconds (default: 600 = 10 minutes)

# Coding LLM - For code generation and debugging tasks
CODING_LLM_BASE_URL=http://localhost:8000/v1
CODING_LLM_MODEL=qwen3-coder
# CODING_LLM_TIMEOUT=300  # Request timeout in seconds (default: 300 = 5 minutes)

# Fast LLM - For quick reasoning tasks (OpenAI-compatible endpoint)
FAST_LLM_URL=http://localhost:11434/v1
FAST_LLM_MODEL=nemotron-3-nano
# FAST_LLM_TIMEOUT=120  # Request timeout in seconds (default: 120 = 2 minutes)

# =============================================================================
# TOOLS
# =============================================================================

# SearXNG Web Search (local instance or any SearXNG endpoint)
SEARXNG_ENDPOINT=http://localhost:8080/search

# Remote Python Executor (sandboxed code execution service)
PYTHON_EXECUTOR_URL=http://pyexec.cluster:9999/

# =============================================================================
# RUNTIME SETTINGS
# =============================================================================

# Fast-path routing for simple queries (bypass orchestration for simple queries)
# When enabled, simple queries like "hello" or "suggest follow-up questions"
# are routed directly to Fast LLM instead of going through the full ReAct loop
FAST_PATH_ENABLED=true

# Maximum orchestration steps before giving up
MAX_ORCHESTRATION_STEPS=10

# Default temperature for orchestrator
ORCHESTRATOR_TEMPERATURE=0.7

# Python executor timeout (seconds)
PYTHON_EXECUTOR_TIMEOUT=30

# Logging level (DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL=INFO

# =============================================================================
# API SERVER
# =============================================================================

# Server host and port
SERVER_HOST=0.0.0.0
SERVER_PORT=8000

# Number of worker processes (for production)
SERVER_WORKERS=1

# Enable auto-reload for development (true/false)
SERVER_RELOAD=false

# =============================================================================
# LANGFUSE OBSERVABILITY
# =============================================================================
# Tracing auto-enables when both keys are provided
# Get keys from https://cloud.langfuse.com or your self-hosted instance

# Langfuse API keys (required for tracing)
LANGFUSE_PUBLIC_KEY=
LANGFUSE_SECRET_KEY=

# Langfuse host (optional, defaults to https://cloud.langfuse.com)
# IMPORTANT: Must include protocol and port, e.g.: http://langfuse.cluster:9999
LANGFUSE_HOST=

# Flush settings (optional)
# LANGFUSE_FLUSH_AT=10
# LANGFUSE_FLUSH_INTERVAL=1.0

# Enable debug logging for Langfuse client (optional)
# LANGFUSE_DEBUG=false
