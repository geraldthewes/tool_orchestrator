# ToolOrchestra Environment Configuration
# Copy this file to .env and update values as needed

# =============================================================================
# ORCHESTRATOR MODEL
# =============================================================================
# The main orchestrator LLM endpoint (vLLM/SGLang/any OpenAI-compatible)
ORCHESTRATOR_BASE_URL=http://localhost:8001/v1
ORCHESTRATOR_MODEL=nvidia/Nemotron-Orchestrator-8B

# =============================================================================
# DELEGATE LLMs
# =============================================================================
# Delegates are configured via config/delegates.yaml
# Environment variables below can override values in the YAML config

# Path to delegates YAML configuration (optional, defaults to config/delegates.yaml)
# DELEGATES_CONFIG_PATH=config/delegates.yaml

# Reasoning LLM - For complex reasoning and analysis tasks
REASONING_LLM_BASE_URL=http://gx10-d8ce.cluster:8000/v1
REASONING_LLM_MODEL=glm-reap

# Coding LLM - For code generation and debugging tasks
CODING_LLM_BASE_URL=http://localhost:8000/v1
CODING_LLM_MODEL=qwen3-coder

# Fast LLM - For quick reasoning tasks (OpenAI-compatible endpoint)
FAST_LLM_URL=http://localhost:11434/v1
FAST_LLM_MODEL=nemotron-3-nano

# =============================================================================
# TOOLS
# =============================================================================

# SearXNG Web Search (local instance or any SearXNG endpoint)
SEARXNG_ENDPOINT=http://localhost:8080/search

# Remote Python Executor (sandboxed code execution service)
PYTHON_EXECUTOR_URL=http://pyexec.cluster:9999/

# =============================================================================
# RUNTIME SETTINGS
# =============================================================================

# Fast-path routing for simple queries (bypass orchestration for simple queries)
# When enabled, simple queries like "hello" or "suggest follow-up questions"
# are routed directly to Fast LLM instead of going through the full ReAct loop
FAST_PATH_ENABLED=true

# Maximum orchestration steps before giving up
MAX_ORCHESTRATION_STEPS=10

# Default temperature for orchestrator
ORCHESTRATOR_TEMPERATURE=0.7

# Python executor timeout (seconds)
PYTHON_EXECUTOR_TIMEOUT=30

# Logging level (DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL=INFO

# =============================================================================
# API SERVER
# =============================================================================

# Server host and port
SERVER_HOST=0.0.0.0
SERVER_PORT=8000

# Number of worker processes (for production)
SERVER_WORKERS=1

# Enable auto-reload for development (true/false)
SERVER_RELOAD=false
